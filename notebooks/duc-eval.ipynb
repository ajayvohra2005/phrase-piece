{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "lambda_client = boto3.client(\"lambda\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e204c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "create_corpus_arn = \"\"\n",
    "\n",
    "def create_corpus(name, s3_uri):\n",
    "\n",
    "  json_data = { \n",
    "    \"CorpusName\": name,\n",
    "    \"S3Uri\": s3_uri,\n",
    "    \"NMax\": 5\n",
    "  }\n",
    "  \n",
    "  payload = json.dumps({ \"body\": json.dumps(json_data) })\n",
    "\n",
    "  response = lambda_client.invoke(\n",
    "      FunctionName='',\n",
    "      InvocationType='RequestResponse',\n",
    "      Payload=payload\n",
    "  )\n",
    "\n",
    "  print(response)\n",
    "  \n",
    "  json_obj = json.loads(response['Payload'].read())\n",
    "  data = json.loads(json_obj['body'])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd3850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "compute_catchphrase_arn = \"\"\n",
    "\n",
    "def compute_catchphrases(corpus_id, tag, top_k=100, affinity=0.10):\n",
    "\n",
    "  json_data = { \n",
    "    \"CorpusId\": corpus_id, \n",
    "    \"Tag\": tag, \n",
    "    \"TopK\": top_k,\n",
    "    \"AffinityThreshold\": affinity\n",
    "  }\n",
    "  \n",
    "  payload = json.dumps({ \"body\": json.dumps(json_data) })\n",
    "\n",
    "  response = lambda_client.invoke(\n",
    "      FunctionName='',\n",
    "      InvocationType='RequestResponse',\n",
    "      Payload=payload\n",
    "  )\n",
    "  \n",
    "  json_obj = json.loads(response['Payload'].read())\n",
    "  data = json.loads(json_obj['body'])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c15a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sfn_client=boto3.client('stepfunctions')\n",
    "def wait_for_sfn_sm(sm_execution_arn):\n",
    "    status = 'RUNNING'\n",
    "    while status == 'RUNNING':\n",
    "        response = sfn_client.describe_execution(executionArn=sm_execution_arn)\n",
    "        status = response.get('status')\n",
    "        if status == 'RUNNING':\n",
    "            time.sleep(15)\n",
    "        \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7783ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_bucket_keys(s3_client, bucket_name, bucket_prefix):\n",
    "    \"\"\"Generator for listing S3 bucket keys matching prefix\"\"\"\n",
    "\n",
    "    kwargs = {'Bucket': bucket_name, 'Prefix': bucket_prefix}\n",
    "    while True:\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            yield obj['Key']\n",
    "\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b166040",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uris = []\n",
    "\n",
    "universe_bucket=\"\"\n",
    "bucket_prefix = \"memray/duc/documents/\"\n",
    "\n",
    "for key in s3_bucket_keys(s3_client=s3_client, bucket_name=universe_bucket, bucket_prefix=bucket_prefix):\n",
    "    s3_uris.append(f\"s3://{universe_bucket}/{key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "sms = []\n",
    "count = 0\n",
    "max_count = 10000\n",
    "\n",
    "filter_names = []\n",
    "for s3_uri in s3_uris:\n",
    "    m=re.match(r\".+\\/id=(\\w+)\\/.+\", s3_uri)\n",
    "    if m:\n",
    "        name = f\"duc-{m[1]}\"\n",
    "        if filter_names and name not in filter_names:\n",
    "            continue\n",
    "        response = create_corpus(name=name, s3_uri=s3_uri)\n",
    "        sms.append( (name, response['CorpusStateMachine'],  response['CorpusId']) )\n",
    "        count += 1\n",
    "    if count >= max_count:\n",
    "        break\n",
    "print(f\"Fast Corpus State Machines running count: {len(sms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ad6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_corpus_failed = []\n",
    "csms = []\n",
    "top_k = 100\n",
    "affinity = 0.10\n",
    "\n",
    "for name, sm, corpus_id in sms:\n",
    "    status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "    if status == \"SUCCEEDED\":\n",
    "        response = compute_catchphrases(corpus_id=corpus_id, tag=name, top_k=top_k, affinity=affinity)\n",
    "        csms.append( (name, response['CorpusStateMachine'], corpus_id) )\n",
    "    else:\n",
    "        create_corpus_failed.append((name, corpus_id))\n",
    "\n",
    "if create_corpus_failed:\n",
    "    print(f\"Fast Corpus Failed: {create_corpus_failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8260e252",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_failed = [ name for name, _ in create_corpus_failed]\n",
    "for s3_uri in s3_uris:\n",
    "    m=re.match(r\".+\\/id=(\\w+)\\/.+\", s3_uri)\n",
    "    if m:\n",
    "        name = f\"duc-{m[1]}\"\n",
    "        if retry_failed and name not in retry_failed:\n",
    "            continue\n",
    "        response = create_corpus(name=name, s3_uri=s3_uri)\n",
    "        sms.append( (name, response['CorpusStateMachine'],  response['CorpusId']) )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_catchphrases_failed = []\n",
    "\n",
    "for name, sm, corpus_id in csms:\n",
    "    status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "    if status != \"SUCCEEDED\":\n",
    "        compute_catchphrases_failed.append( (name, corpus_id) )\n",
    "\n",
    "if compute_catchphrases_failed:\n",
    "    print(f\"Compute Catchphrases Failed: {compute_catchphrases_failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca28044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_catchphrases_failed.append( ('duc-270', '881d3e1e-581a-4327-9dcf-5eb6a3d4da43'))\n",
    "compute_catchphrases_failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19064183",
   "metadata": {},
   "outputs": [],
   "source": [
    "retry_sms = []\n",
    "retry_failed = []\n",
    "for name, corpus_id in compute_catchphrases_failed:\n",
    "    response = compute_catchphrases(corpus_id=corpus_id, tag=name, top_k=top_k, affinity=affinity)\n",
    "    retry_sms.append( (name, response['CorpusStateMachine'], corpus_id) )\n",
    "\n",
    "for name, sm, corpus_id in retry_sms:\n",
    "    status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "    if status != \"SUCCEEDED\":\n",
    "        retry_failed.append( (name, corpus_id) )\n",
    "\n",
    "if retry_failed:\n",
    "    print(retry_failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c851852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "failed = []\n",
    "while retry_failed:\n",
    "    for name, corpus_id in retry_failed:\n",
    "        response = compute_catchphrases(corpus_id=corpus_id, tag=name, top_k=top_k, affinity=affinity)\n",
    "        sm = response['CorpusStateMachine']\n",
    "        status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "        if status != \"SUCCEEDED\":\n",
    "            failed.append( (name, corpus_id) )\n",
    "\n",
    "    if failed:\n",
    "        print(f\"Failed: {failed}\")\n",
    "    retry_failed = failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "import gzip\n",
    "\n",
    "\n",
    "corpora_bucket = \"\"\n",
    "\n",
    "def get_extracted(name):\n",
    "    bucket_prefix = f\"catchphrases/tag={name}/\"\n",
    "        \n",
    "    extracted = []\n",
    "    try:\n",
    "        for key in s3_bucket_keys(s3_client=s3_client, bucket_name=corpora_bucket, bucket_prefix=bucket_prefix):\n",
    "            with NamedTemporaryFile(mode='w+b', delete=True) as file_obj:\n",
    "                s3_client.download_fileobj(corpora_bucket, key, file_obj)\n",
    "                file_obj.seek(0)\n",
    "\n",
    "                with gzip.open(file_obj, mode=\"rb\") as gzip_obj:\n",
    "                    while (line := gzip_obj.readline()):\n",
    "                        json_obj=json.loads(line.decode('utf-8'))\n",
    "                        catchphrase = json_obj['catchphrase']\n",
    "                        weight = json_obj['weight']\n",
    "                        extracted.append( (catchphrase, weight) )\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "    extracted.sort(key = lambda x: x[1], reverse=True)\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_gt_keyphrases(name):\n",
    "    bucket_prefix = f\"memray/duc/keyphrases/id={name}/\"\n",
    "        \n",
    "    gt = []\n",
    "    try:\n",
    "        for key in s3_bucket_keys(s3_client=s3_client, bucket_name=universe_bucket, bucket_prefix=bucket_prefix):\n",
    "            s3_obj = s3_client.get_object(Bucket=universe_bucket, Key=key)\n",
    "            json_str = s3_obj['Body'].read().decode('utf-8')\n",
    "            json_obj = json.loads(json_str)\n",
    "            gt.extend(json_obj['keyphrases'])\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd820489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def normalize_phrases(retrieved, gt):\n",
    "    normalized = retrieved\n",
    "\n",
    "    for p in gt:\n",
    "        i = 0\n",
    "        while i < len(normalized):\n",
    "            s = normalized[i]\n",
    "            idx = s.find(p)\n",
    "            if idx >= 0:\n",
    "                s1 = s[:idx].strip()\n",
    "                s2 = s[idx+len(p):].strip()\n",
    "                insert_list = []\n",
    "                if s1 and s1 not in string.punctuation:\n",
    "                    insert_list.append(s1)\n",
    "                insert_list.append(p)\n",
    "                if s2 and s2 not in string.punctuation:\n",
    "                    insert_list.append(s2)\n",
    "                normalized = normalized[:i] + insert_list + normalized[i+1:]\n",
    "                i = i + len(insert_list)\n",
    "                break\n",
    "            i+=1\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score():\n",
    "\n",
    "    true_positive = []\n",
    "    false_positive = []\n",
    "    false_negative = []\n",
    "\n",
    "    for name, _, _ in sms:\n",
    "        extracted = get_extracted(name)\n",
    "\n",
    "        retrieved = []\n",
    "        for phrase, _ in extracted:\n",
    "            phrase = phrase.lower()\n",
    "            retrieved.append(phrase)\n",
    "        \n",
    "        print(f\"Retrieved: {retrieved}\")\n",
    "        gt_keyphrases = get_gt_keyphrases(name.rsplit('-', 1)[1])\n",
    "        print(f\"Ground Truth: {gt_keyphrases}\")\n",
    "        normalized = normalize_phrases(retrieved=retrieved, gt=gt_keyphrases)\n",
    "        print(f\"Normalized: {normalized}\")\n",
    "\n",
    "        tp = set()\n",
    "        fn = set()\n",
    "        for keyphrase in gt_keyphrases:\n",
    "            if keyphrase in normalized:\n",
    "                tp.add(keyphrase)\n",
    "            else:\n",
    "                fn.add(keyphrase)\n",
    "\n",
    "        true_positive.append(len(tp))\n",
    "        false_positive.append(len(normalized) - len(tp))\n",
    "        false_negative.append(len(fn))\n",
    "    \n",
    "\n",
    "    tp_sum = sum(true_positive)\n",
    "    fp_sum = sum(false_positive)\n",
    "    fn_sum = sum(false_negative)\n",
    "\n",
    "    precision = tp_sum / (tp_sum + fp_sum)\n",
    "    recall = tp_sum / (tp_sum + fn_sum)\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    F2 = 5*precision*recall/(4*precision+recall)\n",
    "\n",
    "    return precision, recall, F1, F2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "precision, recall, F1, F2 = f_score()\n",
    "\n",
    "key = \"FScore/duc270.json\"\n",
    "json_obj = {\n",
    "    \"dataset\": \"duc-270\",\n",
    "    \"topk\": top_k,\n",
    "    \"affinity\": affinity,\n",
    "    \"timestamp\": int(time.time()),\n",
    "    \"F1\": F1,\n",
    "    \"F2\": F2,\n",
    "    \"recall\": recall,\n",
    "    \"precision\": precision\n",
    "}\n",
    "print(json_obj)\n",
    "\n",
    "s3_client.put_object( Bucket=corpora_bucket, Key=key, Body=json.dumps(json_obj))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dbc9917bcaa9a9fa434c727723b90f93ecc3435121eacd019fcd02c268a833c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
