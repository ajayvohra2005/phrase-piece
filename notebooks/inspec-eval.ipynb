{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6983fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "lambda_client = boto3.client(\"lambda\")\n",
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e204c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "create_corpus_arn = \"\"\n",
    "\n",
    "def create_corpus(name, s3_uri):\n",
    "\n",
    "  json_data = { \n",
    "    \"CorpusName\": name,\n",
    "    \"S3Uri\": s3_uri,\n",
    "    \"NMax\": 3\n",
    "  }\n",
    "  \n",
    "  payload = json.dumps({ \"body\": json.dumps(json_data) })\n",
    "\n",
    "  response = lambda_client.invoke(\n",
    "      FunctionName='',\n",
    "      InvocationType='RequestResponse',\n",
    "      Payload=payload\n",
    "  )\n",
    "\n",
    "  print(response)\n",
    "  \n",
    "  json_obj = json.loads(response['Payload'].read())\n",
    "  data = json.loads(json_obj['body'])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c15a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "sfn_client=boto3.client('stepfunctions')\n",
    "def wait_for_sfn_sm(sm_execution_arn):\n",
    "    status = 'RUNNING'\n",
    "    while status == 'RUNNING':\n",
    "        response = sfn_client.describe_execution(executionArn=sm_execution_arn)\n",
    "        status = response.get('status')\n",
    "        if status == 'RUNNING':\n",
    "            time.sleep(15)\n",
    "        \n",
    "    return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7783ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s3_bucket_keys(s3_client, bucket_name, bucket_prefix):\n",
    "    \"\"\"Generator for listing S3 bucket keys matching prefix\"\"\"\n",
    "\n",
    "    kwargs = {'Bucket': bucket_name, 'Prefix': bucket_prefix}\n",
    "    while True:\n",
    "        resp = s3_client.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            yield obj['Key']\n",
    "\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfec8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_corpus(corpus_id):\n",
    "  payload = json.dumps( { \"body\": \"{ \\\"CorpusId\\\": \\\"\" + corpus_id + \"\\\" }\" } )\n",
    "\n",
    "  print(payload)\n",
    "  response = lambda_client.invoke(\n",
    "      FunctionName='',\n",
    "      InvocationType='RequestResponse',\n",
    "      Payload=payload\n",
    "  )\n",
    "\n",
    "  print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b166040",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uris = []\n",
    "\n",
    "universe_bucket=\"\"\n",
    "bucket_prefix = \"midas/inspec/documents/\"\n",
    "\n",
    "for key in s3_bucket_keys(s3_client=s3_client, bucket_name=universe_bucket, bucket_prefix=bucket_prefix):\n",
    "    s3_uris.append(f\"s3://{universe_bucket}/{key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f713b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "time.sleep(3600)\n",
    "\n",
    "sms = []\n",
    "count = 0\n",
    "max_count = 1000\n",
    "\n",
    "filter_names = []\n",
    "for s3_uri in s3_uris:\n",
    "    m=re.match(r\".+\\/id=(\\w+)\\/.+\", s3_uri)\n",
    "    if m:\n",
    "        name = f\"inspec-{m[1]}\"\n",
    "        if filter_names and name not in filter_names:\n",
    "            continue\n",
    "        response = create_corpus(name=name, s3_uri=s3_uri)\n",
    "        sms.append( (name, response['CorpusStateMachine'],  response['CorpusId'], s3_uri) )\n",
    "        count += 1\n",
    "        time.sleep(120)\n",
    "    if count >= max_count:\n",
    "        break\n",
    "print(f\"Fast Corpus State Machines running count: {len(sms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ad6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_corpus_failed = []\n",
    "for name, sm, corpus_id, s3_uri in sms:\n",
    "    status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "    if status != \"SUCCEEDED\":\n",
    "        delete_corpus(corpus_id=corpus_id)\n",
    "        create_corpus_failed.append((name, s3_uri))\n",
    "\n",
    "if create_corpus_failed:\n",
    "    print(f\"Fast Corpus Failed: {create_corpus_failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26281242",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while len(create_corpus_failed) > 0:\n",
    "    sms.clear()\n",
    "    for name, s3_uri in create_corpus_failed:\n",
    "        response = create_corpus(name=name, s3_uri=s3_uri)\n",
    "        sms.append( (name, response['CorpusStateMachine'],  response['CorpusId'], s3_uri) )\n",
    "        time.sleep(60)\n",
    "\n",
    "    create_corpus_failed.clear()\n",
    "    for name, sm, corpus_id, s3_uri in sms:\n",
    "        status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "        if status != \"SUCCEEDED\":\n",
    "            delete_corpus(corpus_id=corpus_id)\n",
    "            create_corpus_failed.append((name, s3_uri))\n",
    "\n",
    "    if create_corpus_failed:\n",
    "        print(f\"Fast corpus Failed: {create_corpus_failed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3b4028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('')\n",
    "\n",
    "response = table.scan()\n",
    "data = response['Items']\n",
    "\n",
    "while 'LastEvaluatedKey' in response:\n",
    "    response = table.scan(ExclusiveStartKey=response['LastEvaluatedKey'])\n",
    "    data.extend(response['Items'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "compute_catchphrase_arn = \"\"\n",
    "\n",
    "def compute_catchphrases(corpus_id, tag, top_k=100, affinity=0.10, n_distance=0.0, c_distance=0.0):\n",
    "\n",
    "  json_data = { \n",
    "    \"CorpusId\": corpus_id, \n",
    "    \"Tag\": tag, \n",
    "    \"TopK\": top_k,\n",
    "    \"AffinityThreshold\": affinity,\n",
    "    \"PosPattern\": \"^(DET)?(((PROPN)|(NOUN))+|(((PROPN)|(NOUN))*((ADJ)|(VERB))+((PROPN)|(NOUN))+)|(((PROPN)|(NOUN)|(ADJ)|(VERB))+((CCONJ)|((ADP)(DET)?))((PROPN)|(NOUN))+))$\",\n",
    "    \"NDistanceThreshold\": n_distance,\n",
    "    \"CDistanceThreshold\": c_distance,\n",
    "  }\n",
    "  \n",
    "  payload = json.dumps({ \"body\": json.dumps(json_data) })\n",
    "\n",
    "  response = lambda_client.invoke(\n",
    "      FunctionName='',\n",
    "      InvocationType='RequestResponse',\n",
    "      Payload=payload\n",
    "  )\n",
    "  \n",
    "  json_obj = json.loads(response['Payload'].read())\n",
    "  data = json.loads(json_obj['body'])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e79ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_catchphrases_list = []\n",
    "csms = []\n",
    "\n",
    "for item in data:\n",
    "  corpus_name = item['corpus_name']\n",
    "  if not corpus_name.startswith(\"semeval2017-\"):\n",
    "     continue\n",
    "  corpus_state = item['corpus_state']\n",
    "\n",
    "  if corpus_state == 'READY':\n",
    "    corpus_id = item['corpus_id']\n",
    "    response = compute_catchphrases(corpus_id=corpus_id, tag=corpus_name)\n",
    "    csms.append( (corpus_name, response['CorpusStateMachine'], corpus_id) )\n",
    "    time.sleep(5)\n",
    "    \n",
    "for name, sm, corpus_id in csms:\n",
    "    status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "    if status != \"SUCCEEDED\":\n",
    "        compute_catchphrases_list.append( (name, corpus_id) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf235a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pending_count = len(compute_catchphrases_list)\n",
    "if pending_count > 0:\n",
    "    print(f\"Pending compute corpus: {pending_count} {compute_catchphrases_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453c3881",
   "metadata": {},
   "outputs": [],
   "source": [
    "while len(compute_catchphrases_list) > 0:\n",
    "    csms.clear()\n",
    "    for name, corpus_id in compute_catchphrases_list:\n",
    "        response = compute_catchphrases(corpus_id=corpus_id, tag=corpus_name)\n",
    "        csms.append( (corpus_name, response['CorpusStateMachine'], corpus_id) )\n",
    "        time.sleep(60)\n",
    "\n",
    "    compute_catchphrases_list.clear()\n",
    "    for name, sm, corpus_id in csms:\n",
    "        status = wait_for_sfn_sm(sm_execution_arn=sm)\n",
    "        if status != \"SUCCEEDED\":\n",
    "            compute_catchphrases_list.append( (name, corpus_id) )\n",
    "\n",
    "    pending_count = len(compute_catchphrases_list)\n",
    "    if pending_count > 0:\n",
    "        print(f\"Pending compute corpus: {pending_count} {compute_catchphrases_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91b6b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "import gzip\n",
    "\n",
    "\n",
    "corpora_bucket = \"\"\n",
    "\n",
    "def get_extracted(name):\n",
    "    bucket_prefix = f\"catchphrases/tag={name}/\"\n",
    "        \n",
    "    extracted = []\n",
    "    try:\n",
    "        for key in s3_bucket_keys(s3_client=s3_client, bucket_name=corpora_bucket, bucket_prefix=bucket_prefix):\n",
    "            with NamedTemporaryFile(mode='w+b', delete=True) as file_obj:\n",
    "                s3_client.download_fileobj(corpora_bucket, key, file_obj)\n",
    "                file_obj.seek(0)\n",
    "\n",
    "                with gzip.open(file_obj, mode=\"rb\") as gzip_obj:\n",
    "                    while (line := gzip_obj.readline()):\n",
    "                        json_obj=json.loads(line.decode('utf-8'))\n",
    "                        catchphrase = json_obj['catchphrase']\n",
    "                        weight = json_obj['weight']\n",
    "                        extracted.append( (catchphrase, weight) )\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "    extracted.sort(key = lambda x: x[1], reverse=True)\n",
    "    return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be5e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "universe_bucket=\"\"\n",
    "\n",
    "def get_gt_keyphrases(name):\n",
    "    bucket_prefix = f\"midas/semeval2017/keyphrases/id={name}/\"\n",
    "        \n",
    "    gt = []\n",
    "    try:\n",
    "        for key in s3_bucket_keys(s3_client=s3_client, bucket_name=universe_bucket, bucket_prefix=bucket_prefix):\n",
    "            s3_obj = s3_client.get_object(Bucket=universe_bucket, Key=key)\n",
    "            json_str = s3_obj['Body'].read().decode('utf-8')\n",
    "            json_obj = json.loads(json_str)\n",
    "            gt.extend(json_obj['keyphrases'])\n",
    "    except KeyError as e:\n",
    "        print(e)\n",
    "\n",
    "    return gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd820489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def normalize_phrases(retrieved, gt):\n",
    "    normalized = retrieved\n",
    "\n",
    "    for p in gt:\n",
    "        i = 0\n",
    "        while i < len(normalized):\n",
    "            s = normalized[i]\n",
    "            idx = s.find(p)\n",
    "            if idx >= 0:\n",
    "                s1 = s[:idx].strip()\n",
    "                s2 = s[idx+len(p):].strip()\n",
    "                insert_list = []\n",
    "                if s1 and s1 not in string.punctuation:\n",
    "                    insert_list.append(s1)\n",
    "                insert_list.append(p)\n",
    "                if s2 and s2 not in string.punctuation:\n",
    "                    insert_list.append(s2)\n",
    "                normalized = normalized[:i] + insert_list + normalized[i+1:]\n",
    "                i = i + len(insert_list)\n",
    "                break\n",
    "            i+=1\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d6a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_score():\n",
    "\n",
    "    true_positive = []\n",
    "    false_positive = []\n",
    "    false_negative = []\n",
    "\n",
    "    for item in data:\n",
    "        name = item['corpus_name']\n",
    "        if not name.startswith(\"semeval2017-\"):\n",
    "            continue\n",
    "        extracted = get_extracted(name)\n",
    "\n",
    "        retrieved = []\n",
    "        for phrase, _ in extracted:\n",
    "            phrase = phrase.lower()\n",
    "            retrieved.append(phrase)\n",
    "        \n",
    "        print(f\"{name}: Retrieved: {retrieved}\")\n",
    "        gt_keyphrases = get_gt_keyphrases(name.rsplit('-', 1)[1])\n",
    "        print(f\"{name}: Ground Truth: {gt_keyphrases}\")\n",
    "        normalized = normalize_phrases(retrieved=retrieved, gt=gt_keyphrases)\n",
    "        print(f\"{name}: Normalized: {normalized}\")\n",
    "\n",
    "        tp = set()\n",
    "        fn = set()\n",
    "        for keyphrase in gt_keyphrases:\n",
    "            if keyphrase in normalized:\n",
    "                tp.add(keyphrase)\n",
    "            else:\n",
    "                fn.add(keyphrase)\n",
    "\n",
    "        true_positive.append(len(tp))\n",
    "        false_positive.append(len(normalized) - len(tp))\n",
    "        false_negative.append(len(fn))\n",
    "    \n",
    "\n",
    "    tp_sum = sum(true_positive)\n",
    "    fp_sum = sum(false_positive)\n",
    "    fn_sum = sum(false_negative)\n",
    "\n",
    "    precision = tp_sum / (tp_sum + fp_sum)\n",
    "    recall = tp_sum / (tp_sum + fn_sum)\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    F2 = 5*precision*recall/(4*precision+recall)\n",
    "\n",
    "    return precision, recall, F1, F2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3c5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "precision, recall, F1, F2 = f_score()\n",
    "\n",
    "key = \"FScore/semeval2017.json\"\n",
    "json_obj = {\n",
    "    \"dataset\": \"semeval2017\",\n",
    "    \"topk\": 1000,\n",
    "    \"affinity\": 0.01,\n",
    "    \"distance\": 0.10,\n",
    "    \"max_length\": 5,\n",
    "    \"timestamp\": int(time.time()),\n",
    "    \"F1\": F1,\n",
    "    \"F2\": F2,\n",
    "    \"recall\": recall,\n",
    "    \"precision\": precision\n",
    "}\n",
    "print(json_obj)\n",
    "\n",
    "s3_client.put_object( Bucket=corpora_bucket, Key=key, Body=json.dumps(json_obj))\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "4dbc9917bcaa9a9fa434c727723b90f93ecc3435121eacd019fcd02c268a833c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
